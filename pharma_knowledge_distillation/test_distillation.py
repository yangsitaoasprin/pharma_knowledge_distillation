import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.training.evaluator import ModelEvaluator
from src.models.teacher_model import TeacherModel
from src.models.student_model import StudentModel
import time

# åˆå§‹åŒ–æ¨¡å‹
teacher = TeacherModel('deepseek-r1:latest')
student = StudentModel('qwen2:0.5b')
evaluator = ModelEvaluator(teacher, student)

# æµ‹è¯•é—®é¢˜
test_questions = [
    'é˜¿å¸åŒ¹æ—æ˜¯ä»€ä¹ˆè¯ç‰©ï¼Ÿ',
    'ç»´ç”Ÿç´ Dç¼ºä¹å¦‚ä½•è¡¥å……ï¼Ÿ',
    'æ°¯åŒ–é’¾çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ'
]

print('ğŸ§ª çŸ¥è¯†è’¸é¦æµ‹è¯•å¼€å§‹...')
print('='*60)

for question in test_questions:
    print(f'\nâ“ é—®é¢˜: {question}')
    
    # è·å–æ•™å¸ˆå›ç­”
    teacher_response = teacher.generate_response(question)
    print(f'ğŸ‘¨â€ğŸ« æ•™å¸ˆå›ç­”: {teacher_response.text[:100]}...')
    
    # è·å–å­¦ç”Ÿå›ç­”  
    student_response = student.generate_response(question)
    print(f'ğŸ‘¨â€ğŸ“ å­¦ç”Ÿå›ç­”: {student_response.text[:100]}...')
    
    # è¯„ä¼°ç›¸ä¼¼åº¦
    result = evaluator.evaluate_single_response(question, teacher_response.text)
    
    print(f'ğŸ“Š ç›¸ä¼¼åº¦: {result["metrics"]["similarity_to_teacher"]:.3f}')
    print(f'â­ è´¨é‡åˆ†: {result["metrics"]["response_quality"]:.3f}')
    print(f'ğŸ¯ å­¦ç”Ÿç½®ä¿¡åº¦: {result["metrics"]["student_confidence"]:.3f}')
    print('-'*50)
    
    time.sleep(1)  # é¿å…è¿‡å¿«è¯·æ±‚

print('\nâœ… æµ‹è¯•å®Œæˆï¼')